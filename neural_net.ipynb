{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from uszipcode import SearchEngine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Class and Training Functions\n",
    "Define Class and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nn_model, X_train, y_train, X_eval, y_eval, k, max_iter=50, batch_size=32, print_n=10, verbose=True):\n",
    "    '''\n",
    "    Trains neural network model on X_train, y_train data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.array\n",
    "        matrix of training data features\n",
    "    y_train: np.array\n",
    "        vector of training data labels\n",
    "    k: int\n",
    "        size of hidden layer to use in neural network\n",
    "    max_iter: int\n",
    "        maximum number of iterations to train for\n",
    "    batch_size: int\n",
    "        batch size to use when training w/ SGD\n",
    "    print_n: int\n",
    "        print training progress every print_n steps\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    nn_model: torch.nn.Module\n",
    "        trained neural network model\n",
    "    '''\n",
    "    # convert to tensors (for Pytorch)\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    y_train_tensor = torch.tensor(y_train)\n",
    "    X_test_tensor = torch.tensor(X_eval)\n",
    "    y_test_tensor = torch.tensor(y_eval)\n",
    "    # intialize neural network\n",
    "    n_samples, n_features = X_train_tensor.shape\n",
    "    #nn_model = NN(n_features, k)\n",
    "    nn_model.train()  # put model in train mode\n",
    "    # initialize mse loss function\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    # train with (mini-batch) SGD; initialize optimizer\n",
    "    opt = torch.optim.SGD(nn_model.parameters(), lr=1e-4)\n",
    "    losses_test = []\n",
    "    for it in range(max_iter):\n",
    "        # save losses across all batches\n",
    "        losses = []\n",
    "        # loop through data in batches\n",
    "        for batch_start in range(0, n_samples, batch_size):\n",
    "            # reset gradients to zero\n",
    "            opt.zero_grad()\n",
    "            # form batch\n",
    "            X_batch = X_train_tensor[batch_start:batch_start+batch_size]\n",
    "            y_batch = y_train_tensor[batch_start:batch_start+batch_size]\n",
    "            X_batch_test = X_test_tensor[batch_start:batch_start+batch_size]\n",
    "            y_batch_test = y_test_tensor[batch_start:batch_start+batch_size]\n",
    "            # pass batch through neural net to get prediction\n",
    "            y_pred = nn_model(X_batch.float())\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "            y_pred_test = nn_model(X_batch_test.float())\n",
    "            y_pred_test = y_pred_test.unsqueeze(1)\n",
    "            #print(y_pred)\n",
    "            # compute MSE loss\n",
    "            loss = mse_loss(y_pred, y_batch[:, None].float())\n",
    "            loss_test = mse_loss(y_pred_test, y_batch_test[:, None].float())\n",
    "            # back-propagate loss\n",
    "            loss.backward()\n",
    "            # update model parameters based on backpropogated gradients\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "            losses_test.append(loss.item())\n",
    "        if verbose and it % print_n == 0:\n",
    "            print(f\"Mean Train MSE at step {it}: {np.mean(losses)}\")\n",
    "    return nn_model, losses_test\n",
    "\n",
    "def evaluate_model(nn_model, X_eval, y_eval, batch_size=32):\n",
    "    '''\n",
    "    Evaluates trained neural network model on X_eval, y_eval data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_model: torch.nn.Module\n",
    "        trained neural network model\n",
    "    X_eval: np.array\n",
    "        matrix of training data features\n",
    "    y_eval: np.array\n",
    "        vector of training data labels\n",
    "    batch_size: int\n",
    "        batch size to looping over dataset to generate predictions\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse: float\n",
    "        MSE of trained model on X_eval, y_eval data\n",
    "    '''\n",
    "    # initialize mse loss function\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    # convert to tensors (for Pytorch)\n",
    "    X_eval_tensor = torch.tensor(X_eval)\n",
    "    y_eval_tensor = torch.tensor(y_eval)\n",
    "    n_samples = X_eval_tensor.shape[0]\n",
    "    nn_model.eval() # put in eval mode\n",
    "    # loop over data and generate predictions\n",
    "    preds = []\n",
    "    for batch_start in range(0, n_samples, batch_size):\n",
    "        # form batch\n",
    "        X_batch = X_eval_tensor[batch_start:batch_start+batch_size]\n",
    "        y_batch = y_eval_tensor[batch_start:batch_start+batch_size]\n",
    "        with torch.no_grad():  # no need to compute gradients during evaluation\n",
    "            # pass batch through neural net to get prediction\n",
    "            y_pred = nn_model(X_batch.float())\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "            preds.append(y_pred)\n",
    "    # compute MSE across all samples\n",
    "    all_preds = torch.cat(preds)\n",
    "    loss = mse_loss(all_preds, y_eval_tensor[:, None].float()).item()\n",
    "    return loss\n",
    "\n",
    "class NN(nn.Module):\n",
    "    '''\n",
    "    Class for fully connected neural net.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            input dimension (i.e., # of features in each example passed to the network)\n",
    "        hidden_dim: int\n",
    "            number of nodes in hidden layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            # Network has a single hidden layer\n",
    "            # Apply ReLU activation in between the hidden layer and output node\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NN_configureable(nn.Module):\n",
    "    '''\n",
    "    Class for fully connected neural net.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_layers):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            input dimension (i.e., # of features in each example passed to the network)\n",
    "        hidden_dim: int\n",
    "            number of nodes in hidden layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        # Define input layer\n",
    "        self.layers[\"input\"] = nn.Linear(in_features = input_dim, out_features = hidden_dim)\n",
    "        # Define hidden layers\n",
    "        for i in range(self.hidden_layers):\n",
    "            self.layers[f\"hidden_{i}\"] = nn.Linear(in_features = hidden_dim, out_features = hidden_dim)\n",
    "        # Define output layer\n",
    "        self.layers[\"output\"] = nn.Linear(in_features = hidden_dim, out_features = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[\"input\"](x)\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = F.relu(self.layers[f\"hidden_{i}\"](x))\n",
    "\n",
    "        return self.layers[\"output\"](x)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "Get data from different sources before combining\n",
    "* Cleaned up EV data: TX_WA_CO_NY.csv\n",
    "* Average EV price and new car data over time: Avg_EV_Price.csv\n",
    "* Census data (pop, household income, zipcode): census.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():   \n",
    "    # Import data\n",
    "    X = pd.read_csv('./Data/df_X_county.csv')\n",
    "    X['constant'] = 1\n",
    "    y = pd.read_csv('./Data/df_y_county.csv')\n",
    "\n",
    "    # check if any nan values\n",
    "    nan_row_X = X[X.isna().any(axis=1)]\n",
    "    #print(nan_row_X)\n",
    "    nan_row_y = y[y.isna().any(axis=1)]\n",
    "    #print(nan_row_y)\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    # split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # standardize X\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    #print(X_test.shape)\n",
    "    #print(y_test.shape)\n",
    "    \n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    # train NN model to predict EV registration using train data\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    n_samples, input_dim = X_train_tensor.shape\n",
    "    n_layers = np.arange(1, 5) # iterate through hidden layer count\n",
    "    n_hidden_dim = np.arange(8, 65, 8)  # iterate through hidden layer node count\n",
    "    mse_dict = {}\n",
    "    \n",
    "    for i in range(len(n_layers)):\n",
    "        for j in range(len(n_hidden_dim)):\n",
    "            tuple_place = (i, j)\n",
    "            nn_model_place = NN_configureable(input_dim, hidden_dim = j, hidden_layers = i)\n",
    "            nn_model_result = train_model(nn_model_place, X_train, y_train, X_test, y_test, 32)\n",
    "            train_mse =  evaluate_model(nn_model_result[0], X_train, y_train)\n",
    "            test_mse = evaluate_model(nn_model_result[0], X_test, y_test)\n",
    "            #print(tuple_place)\n",
    "            train_test_list = [train_mse, test_mse]\n",
    "            mse_dict[tuple_place] = train_test_list\n",
    "            \n",
    "        \n",
    "        \n",
    "    #nn_model_place = NN_configureable(input_dim, 8, 2)\n",
    "    #nn_model_place = NN(input_dim, 8)\n",
    "    #nn_model_result = train_model(nn_model_place, X_train, y_train, X_test, y_test, 32)\n",
    "    #train_mse = evaluate_model(nn_model_result[0], X_train, y_train)\n",
    "    #test_mse = evaluate_model(nn_model_result[0], X_test, y_test)\n",
    "        \n",
    "    for key in mse_dict:\n",
    "        print(f\"Train MSE for model: hidden_layers = {key[0]}, hidden_dim = {key[1]} is: {mse_dict[key][0]}\")\n",
    "        print(f\"Test MSE for model: hidden_layers = {key[0]}, hidden_dim = {key[1]} is: {mse_dict[key][1]}\")\n",
    "\n",
    "    # plot the model's test errors\n",
    "    #plt.plot(range(len(nn_model_result[1])), nn_model_result[1])\n",
    "    # axis labels\n",
    "    plt.xlabel('Iteration Step')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.title(\"Model 1 - Hidden Layer - ReLU\")\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train MSE at step 0: 1311284.575873863\n",
      "Mean Train MSE at step 10: 1204055.263167547\n",
      "Mean Train MSE at step 20: 1184544.1300616197\n",
      "Mean Train MSE at step 30: 1180992.9472931337\n",
      "Mean Train MSE at step 40: 1180346.2403169014\n",
      "Mean Train MSE at step 0: 886017.6626182878\n",
      "Mean Train MSE at step 10: 855231.2307896861\n",
      "Mean Train MSE at step 20: 850054.980445826\n",
      "Mean Train MSE at step 30: 847394.4965338908\n",
      "Mean Train MSE at step 40: 845582.9441479606\n",
      "Mean Train MSE at step 0: 841202.4186427157\n",
      "Mean Train MSE at step 10: 831394.9572559053\n",
      "Mean Train MSE at step 20: 832681.7341274208\n",
      "Mean Train MSE at step 30: 834723.7423296288\n",
      "Mean Train MSE at step 40: 835635.2332608935\n",
      "Mean Train MSE at step 0: 859304.9757188967\n",
      "Mean Train MSE at step 10: 830709.0645860842\n",
      "Mean Train MSE at step 20: 825983.9869195643\n",
      "Mean Train MSE at step 30: 822385.8489354093\n",
      "Mean Train MSE at step 40: 819139.6531231661\n",
      "Mean Train MSE at step 0: 880330.3391239364\n",
      "Mean Train MSE at step 10: 832980.8469410212\n",
      "Mean Train MSE at step 20: 826118.3311119975\n",
      "Mean Train MSE at step 30: 822370.618865537\n",
      "Mean Train MSE at step 40: 819174.0556457232\n",
      "Mean Train MSE at step 0: 858265.4982486062\n",
      "Mean Train MSE at step 10: 832421.8194001247\n",
      "Mean Train MSE at step 20: 826084.1456957894\n",
      "Mean Train MSE at step 30: 822438.157652949\n",
      "Mean Train MSE at step 40: 819189.1856660799\n",
      "Mean Train MSE at step 0: 857692.9980422902\n",
      "Mean Train MSE at step 10: 831707.7240821229\n",
      "Mean Train MSE at step 20: 826053.0242237933\n",
      "Mean Train MSE at step 30: 822386.1785504695\n",
      "Mean Train MSE at step 40: 819159.0911825118\n",
      "Mean Train MSE at step 0: 860632.5595198798\n",
      "Mean Train MSE at step 10: 836336.4776628522\n",
      "Mean Train MSE at step 20: 826349.3869571596\n",
      "Mean Train MSE at step 30: 822132.785087478\n",
      "Mean Train MSE at step 40: 819151.2477213541\n",
      "Mean Train MSE at step 0: 1311284.575873863\n",
      "Mean Train MSE at step 10: 1204055.263167547\n",
      "Mean Train MSE at step 20: 1184544.1300616197\n",
      "Mean Train MSE at step 30: 1180992.9472931337\n",
      "Mean Train MSE at step 40: 1180346.2403169014\n",
      "Mean Train MSE at step 0: nan\n",
      "Mean Train MSE at step 10: nan\n",
      "Mean Train MSE at step 20: nan\n",
      "Mean Train MSE at step 30: nan\n",
      "Mean Train MSE at step 40: nan\n",
      "Mean Train MSE at step 0: nan\n",
      "Mean Train MSE at step 10: nan\n",
      "Mean Train MSE at step 20: nan\n",
      "Mean Train MSE at step 30: nan\n",
      "Mean Train MSE at step 40: nan\n",
      "Mean Train MSE at step 0: nan\n",
      "Mean Train MSE at step 10: nan\n",
      "Mean Train MSE at step 20: nan\n",
      "Mean Train MSE at step 30: nan\n",
      "Mean Train MSE at step 40: nan\n",
      "Mean Train MSE at step 0: nan\n",
      "Mean Train MSE at step 10: nan\n",
      "Mean Train MSE at step 20: nan\n",
      "Mean Train MSE at step 30: nan\n",
      "Mean Train MSE at step 40: nan\n",
      "Mean Train MSE at step 0: nan\n",
      "Mean Train MSE at step 10: nan\n",
      "Mean Train MSE at step 20: nan\n",
      "Mean Train MSE at step 30: nan\n",
      "Mean Train MSE at step 40: nan\n",
      "Mean Train MSE at step 0: nan\n",
      "Mean Train MSE at step 10: nan\n",
      "Mean Train MSE at step 20: nan\n",
      "Mean Train MSE at step 30: nan\n",
      "Mean Train MSE at step 40: nan\n",
      "Mean Train MSE at step 0: nan\n",
      "Mean Train MSE at step 10: nan\n",
      "Mean Train MSE at step 20: nan\n",
      "Mean Train MSE at step 30: nan\n",
      "Mean Train MSE at step 40: nan\n",
      "Mean Train MSE at step 0: 1311284.575873863\n",
      "Mean Train MSE at step 10: 1204055.263167547\n",
      "Mean Train MSE at step 20: 1184544.1300616197\n",
      "Mean Train MSE at step 30: 1180992.9472931337\n",
      "Mean Train MSE at step 40: 1180346.2403169014\n",
      "Mean Train MSE at step 0: 1119294.503539466\n",
      "Mean Train MSE at step 10: 1112298.4720510563\n",
      "Mean Train MSE at step 20: 1110169.1794857688\n",
      "Mean Train MSE at step 30: 1109348.4508325998\n",
      "Mean Train MSE at step 40: 1109027.600755575\n",
      "Mean Train MSE at step 0: 1311863.5137131382\n",
      "Mean Train MSE at step 10: 1204136.9903994277\n",
      "Mean Train MSE at step 20: 1184558.9135214936\n",
      "Mean Train MSE at step 30: 1180995.6119608274\n",
      "Mean Train MSE at step 40: 1180346.7391615317\n",
      "Mean Train MSE at step 0: 1526462.6779957085\n",
      "Mean Train MSE at step 10: 1204256.107949127\n",
      "Mean Train MSE at step 20: 1184580.6267513938\n",
      "Mean Train MSE at step 30: 1180999.60546875\n",
      "Mean Train MSE at step 40: 1180347.4612859448\n",
      "Mean Train MSE at step 0: 1430378.5268417143\n",
      "Mean Train MSE at step 10: 1204143.8869342357\n",
      "Mean Train MSE at step 20: 1184560.1998789613\n",
      "Mean Train MSE at step 30: 1180995.8620525235\n",
      "Mean Train MSE at step 40: 1180346.7645980048\n",
      "Mean Train MSE at step 0: 1311360.5168422644\n",
      "Mean Train MSE at step 10: 1204023.0217227847\n",
      "Mean Train MSE at step 20: 1184538.1565159184\n",
      "Mean Train MSE at step 30: 1180991.8536715081\n",
      "Mean Train MSE at step 40: 1180346.0419967724\n",
      "Mean Train MSE at step 0: 9551233.038319763\n",
      "Mean Train MSE at step 10: 1204654.2641166006\n",
      "Mean Train MSE at step 20: 1184653.0700740903\n",
      "Mean Train MSE at step 30: 1181012.8243471244\n",
      "Mean Train MSE at step 40: 1180349.8709286973\n",
      "Mean Train MSE at step 0: 2682698.542430036\n",
      "Mean Train MSE at step 10: 1204102.9814453125\n",
      "Mean Train MSE at step 20: 1184552.687912632\n",
      "Mean Train MSE at step 30: 1180994.507078932\n",
      "Mean Train MSE at step 40: 1180346.516982101\n",
      "Mean Train MSE at step 0: 1311284.575873863\n",
      "Mean Train MSE at step 10: 1204055.263167547\n",
      "Mean Train MSE at step 20: 1184544.1300616197\n",
      "Mean Train MSE at step 30: 1180992.9472931337\n",
      "Mean Train MSE at step 40: 1180346.2403169014\n",
      "Mean Train MSE at step 0: 1311894.1388781911\n",
      "Mean Train MSE at step 10: 1204166.2063847932\n",
      "Mean Train MSE at step 20: 1184564.3303898915\n",
      "Mean Train MSE at step 30: 1180996.6508766138\n",
      "Mean Train MSE at step 40: 1180346.9155846538\n",
      "Mean Train MSE at step 0: 1311236.5564618178\n",
      "Mean Train MSE at step 10: 1204046.5476727553\n",
      "Mean Train MSE at step 20: 1184542.5057860182\n",
      "Mean Train MSE at step 30: 1180992.6454298708\n",
      "Mean Train MSE at step 40: 1180346.17325044\n",
      "Mean Train MSE at step 0: 1322455.9797718604\n",
      "Mean Train MSE at step 10: 1204122.9664805238\n",
      "Mean Train MSE at step 20: 1184556.3005886883\n",
      "Mean Train MSE at step 30: 1180995.1627604167\n",
      "Mean Train MSE at step 40: 1180346.6287778756\n",
      "Mean Train MSE at step 0: 154886064.0864212\n",
      "Mean Train MSE at step 10: 1206815.3762929137\n",
      "Mean Train MSE at step 20: 1185046.298919821\n",
      "Mean Train MSE at step 30: 1181084.3945495891\n",
      "Mean Train MSE at step 40: 1180362.9323466842\n",
      "Mean Train MSE at step 0: 3.965336889474703e+19\n",
      "Mean Train MSE at step 10: 60226435026540.17\n",
      "Mean Train MSE at step 20: 10956577095141.559\n",
      "Mean Train MSE at step 30: 1993255925423.474\n",
      "Mean Train MSE at step 40: 362619678969.9906\n",
      "Mean Train MSE at step 0: 1311184.7366994938\n",
      "Mean Train MSE at step 10: 1204035.7760875146\n",
      "Mean Train MSE at step 20: 1183961.7748312794\n",
      "Mean Train MSE at step 30: 1180886.9329885563\n",
      "Mean Train MSE at step 40: 1180326.9189957452\n",
      "Mean Train MSE at step 0: 198914350.71713844\n",
      "Mean Train MSE at step 10: 1207628.2894476233\n",
      "Mean Train MSE at step 20: 1185194.3437133215\n",
      "Mean Train MSE at step 30: 1181111.3431631455\n",
      "Mean Train MSE at step 40: 1180367.8198356808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE for model: hidden_layers = 0, hidden_dim = 0 is: 1182471.125\n",
      "Test MSE for model: hidden_layers = 0, hidden_dim = 0 is: 1115955.875\n",
      "Train MSE for model: hidden_layers = 0, hidden_dim = 1 is: 731413.875\n",
      "Test MSE for model: hidden_layers = 0, hidden_dim = 1 is: 709449.0625\n",
      "Train MSE for model: hidden_layers = 0, hidden_dim = 2 is: 737443.875\n",
      "Test MSE for model: hidden_layers = 0, hidden_dim = 2 is: 713826.875\n",
      "Train MSE for model: hidden_layers = 0, hidden_dim = 3 is: 746071.625\n",
      "Test MSE for model: hidden_layers = 0, hidden_dim = 3 is: 720078.4375\n",
      "Train MSE for model: hidden_layers = 0, hidden_dim = 4 is: 746073.4375\n",
      "Test MSE for model: hidden_layers = 0, hidden_dim = 4 is: 720081.625\n",
      "Train MSE for model: hidden_layers = 0, hidden_dim = 5 is: 746097.4375\n",
      "Test MSE for model: hidden_layers = 0, hidden_dim = 5 is: 720101.375\n",
      "Train MSE for model: hidden_layers = 0, hidden_dim = 6 is: 746076.875\n",
      "Test MSE for model: hidden_layers = 0, hidden_dim = 6 is: 720083.375\n",
      "Train MSE for model: hidden_layers = 0, hidden_dim = 7 is: 746038.375\n",
      "Test MSE for model: hidden_layers = 0, hidden_dim = 7 is: 720052.6875\n",
      "Train MSE for model: hidden_layers = 1, hidden_dim = 0 is: 1182471.125\n",
      "Test MSE for model: hidden_layers = 1, hidden_dim = 0 is: 1115955.875\n",
      "Train MSE for model: hidden_layers = 1, hidden_dim = 1 is: nan\n",
      "Test MSE for model: hidden_layers = 1, hidden_dim = 1 is: nan\n",
      "Train MSE for model: hidden_layers = 1, hidden_dim = 2 is: nan\n",
      "Test MSE for model: hidden_layers = 1, hidden_dim = 2 is: nan\n",
      "Train MSE for model: hidden_layers = 1, hidden_dim = 3 is: nan\n",
      "Test MSE for model: hidden_layers = 1, hidden_dim = 3 is: nan\n",
      "Train MSE for model: hidden_layers = 1, hidden_dim = 4 is: nan\n",
      "Test MSE for model: hidden_layers = 1, hidden_dim = 4 is: nan\n",
      "Train MSE for model: hidden_layers = 1, hidden_dim = 5 is: nan\n",
      "Test MSE for model: hidden_layers = 1, hidden_dim = 5 is: nan\n",
      "Train MSE for model: hidden_layers = 1, hidden_dim = 6 is: nan\n",
      "Test MSE for model: hidden_layers = 1, hidden_dim = 6 is: nan\n",
      "Train MSE for model: hidden_layers = 1, hidden_dim = 7 is: nan\n",
      "Test MSE for model: hidden_layers = 1, hidden_dim = 7 is: nan\n",
      "Train MSE for model: hidden_layers = 2, hidden_dim = 0 is: 1182471.125\n",
      "Test MSE for model: hidden_layers = 2, hidden_dim = 0 is: 1115955.875\n",
      "Train MSE for model: hidden_layers = 2, hidden_dim = 1 is: 1116117.75\n",
      "Test MSE for model: hidden_layers = 2, hidden_dim = 1 is: 1057916.375\n",
      "Train MSE for model: hidden_layers = 2, hidden_dim = 2 is: 1182471.25\n",
      "Test MSE for model: hidden_layers = 2, hidden_dim = 2 is: 1115955.75\n",
      "Train MSE for model: hidden_layers = 2, hidden_dim = 3 is: 1182471.5\n",
      "Test MSE for model: hidden_layers = 2, hidden_dim = 3 is: 1115955.5\n",
      "Train MSE for model: hidden_layers = 2, hidden_dim = 4 is: 1182471.25\n",
      "Test MSE for model: hidden_layers = 2, hidden_dim = 4 is: 1115955.75\n",
      "Train MSE for model: hidden_layers = 2, hidden_dim = 5 is: 1182471.125\n",
      "Test MSE for model: hidden_layers = 2, hidden_dim = 5 is: 1115956.0\n",
      "Train MSE for model: hidden_layers = 2, hidden_dim = 6 is: 1182471.875\n",
      "Test MSE for model: hidden_layers = 2, hidden_dim = 6 is: 1115954.75\n",
      "Train MSE for model: hidden_layers = 2, hidden_dim = 7 is: 1182471.125\n",
      "Test MSE for model: hidden_layers = 2, hidden_dim = 7 is: 1115955.75\n",
      "Train MSE for model: hidden_layers = 3, hidden_dim = 0 is: 1182471.125\n",
      "Test MSE for model: hidden_layers = 3, hidden_dim = 0 is: 1115955.875\n",
      "Train MSE for model: hidden_layers = 3, hidden_dim = 1 is: 1182471.25\n",
      "Test MSE for model: hidden_layers = 3, hidden_dim = 1 is: 1115955.75\n",
      "Train MSE for model: hidden_layers = 3, hidden_dim = 2 is: 1182471.125\n",
      "Test MSE for model: hidden_layers = 3, hidden_dim = 2 is: 1115955.75\n",
      "Train MSE for model: hidden_layers = 3, hidden_dim = 3 is: 1182471.25\n",
      "Test MSE for model: hidden_layers = 3, hidden_dim = 3 is: 1115955.875\n",
      "Train MSE for model: hidden_layers = 3, hidden_dim = 4 is: 1182474.75\n",
      "Test MSE for model: hidden_layers = 3, hidden_dim = 4 is: 1115950.75\n",
      "Train MSE for model: hidden_layers = 3, hidden_dim = 5 is: 71736090624.0\n",
      "Test MSE for model: hidden_layers = 3, hidden_dim = 5 is: 71744004096.0\n",
      "Train MSE for model: hidden_layers = 3, hidden_dim = 6 is: 1182466.875\n",
      "Test MSE for model: hidden_layers = 3, hidden_dim = 6 is: 1115962.75\n",
      "Train MSE for model: hidden_layers = 3, hidden_dim = 7 is: 1182475.875\n",
      "Test MSE for model: hidden_layers = 3, hidden_dim = 7 is: 1115949.25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
