{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "from uszipcode import SearchEngine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Class and Training Functions\n",
    "Define Class and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nn_model, X_train, y_train, X_eval, y_eval, k, max_iter=50, batch_size=32, print_n=10, verbose=False):\n",
    "    '''\n",
    "    Trains neural network model on X_train, y_train data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.array\n",
    "        matrix of training data features\n",
    "    y_train: np.array\n",
    "        vector of training data labels\n",
    "    k: int\n",
    "        size of hidden layer to use in neural network\n",
    "    max_iter: int\n",
    "        maximum number of iterations to train for\n",
    "    batch_size: int\n",
    "        batch size to use when training w/ SGD\n",
    "    print_n: int\n",
    "        print training progress every print_n steps\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    nn_model: torch.nn.Module\n",
    "        trained neural network model\n",
    "    '''\n",
    "    # convert to tensors (for Pytorch)\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    y_train_tensor = torch.tensor(y_train)\n",
    "    X_test_tensor = torch.tensor(X_eval)\n",
    "    y_test_tensor = torch.tensor(y_eval)\n",
    "    # intialize neural network\n",
    "    n_samples, n_features = X_train_tensor.shape\n",
    "    #nn_model = NN(n_features, k)\n",
    "    nn_model.train()  # put model in train mode\n",
    "    # initialize mse loss function\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    # train with (mini-batch) SGD; initialize optimizer\n",
    "    opt = torch.optim.SGD(nn_model.parameters(), lr=1e-4)\n",
    "    losses_test = []\n",
    "    for it in range(max_iter):\n",
    "        # save losses across all batches\n",
    "        losses = []\n",
    "        # loop through data in batches\n",
    "        for batch_start in range(0, n_samples, batch_size):\n",
    "            # reset gradients to zero\n",
    "            opt.zero_grad()\n",
    "            # form batch\n",
    "            X_batch = X_train_tensor[batch_start:batch_start+batch_size]\n",
    "            y_batch = y_train_tensor[batch_start:batch_start+batch_size]\n",
    "            X_batch_test = X_test_tensor[batch_start:batch_start+batch_size]\n",
    "            y_batch_test = y_test_tensor[batch_start:batch_start+batch_size]\n",
    "            # pass batch through neural net to get prediction\n",
    "            y_pred = nn_model(X_batch.float())\n",
    "            y_pred_test = nn_model(X_batch_test.float())\n",
    "            # compute MSE loss\n",
    "            loss = mse_loss(y_pred, y_batch[:, None].float())\n",
    "            loss_test = mse_loss(y_pred_test, y_batch_test[:, None].float())\n",
    "            # back-propagate loss\n",
    "            loss.backward()\n",
    "            # update model parameters based on backpropogated gradients\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "            losses_test.append(loss.item())\n",
    "        if verbose and it % print_n == 0:\n",
    "            print(f\"Mean Train MSE at step {it}: {np.mean(losses)}\")\n",
    "    return nn_model, losses_test\n",
    "\n",
    "def evaluate_model(nn_model, X_eval, y_eval, batch_size=32):\n",
    "    '''\n",
    "    Evaluates trained neural network model on X_eval, y_eval data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_model: torch.nn.Module\n",
    "        trained neural network model\n",
    "    X_eval: np.array\n",
    "        matrix of training data features\n",
    "    y_eval: np.array\n",
    "        vector of training data labels\n",
    "    batch_size: int\n",
    "        batch size to looping over dataset to generate predictions\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse: float\n",
    "        MSE of trained model on X_eval, y_eval data\n",
    "    '''\n",
    "    # initialize mse loss function\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    # convert to tensors (for Pytorch)\n",
    "    X_eval_tensor = torch.tensor(X_eval)\n",
    "    y_eval_tensor = torch.tensor(y_eval)\n",
    "    n_samples = X_eval_tensor.shape[0]\n",
    "    nn_model.eval() # put in eval mode\n",
    "    # loop over data and generate predictions\n",
    "    preds = []\n",
    "    for batch_start in range(0, n_samples, batch_size):\n",
    "        # form batch\n",
    "        X_batch = X_eval_tensor[batch_start:batch_start+batch_size]\n",
    "        y_batch = y_eval_tensor[batch_start:batch_start+batch_size]\n",
    "        with torch.no_grad():  # no need to compute gradients during evaluation\n",
    "            # pass batch through neural net to get prediction\n",
    "            y_pred = nn_model(X_batch.float())\n",
    "            preds.append(y_pred)\n",
    "    # compute MSE across all samples\n",
    "    all_preds = torch.cat(preds)\n",
    "    loss = mse_loss(all_preds, y_eval_tensor[:, None].float()).item()\n",
    "    return loss\n",
    "\n",
    "class NN1(nn.Module):\n",
    "    '''\n",
    "    Class for fully connected neural net.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            input dimension (i.e., # of features in each example passed to the network)\n",
    "        hidden_dim: int\n",
    "            number of nodes in hidden layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.ReLU()\n",
    "        )    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "Get data from different sources before combining\n",
    "* Cleaned up EV data: TX_WA_CO_NY.csv\n",
    "* Average EV price and new car data over time: Avg_EV_Price.csv\n",
    "* Census data (pop, household income, zipcode): census.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():   \n",
    "    # Import data\n",
    "    df_reg = pd.read_csv('./Data/TX_WA_CO_NY.csv')\n",
    "    df_ev = pd.read_csv('./Data/Avg_EV_Price.csv')\n",
    "    df_c = pd.read_csv('./Data/Census Data/census.csv')\n",
    "\n",
    "    # Convert dates to datetime dtype\n",
    "    df_reg['Registration Date'] = pd.to_datetime(df_reg['Registration Date'])\n",
    "    df_ev['Month'] = pd.to_datetime(df_ev['Month'], format='%b-%y')\n",
    "\n",
    "    #Aggregate by County\n",
    "    # create a SearchEngine object\n",
    "    search = SearchEngine()\n",
    "\n",
    "    # define a function to map zip codes to counties\n",
    "    def zipcode_to_county(zipcode):\n",
    "        #This county does not get populated for some reason\n",
    "        if zipcode == 75033:\n",
    "            return \"Collin County\"\n",
    "        \n",
    "        zipcode_data = search.by_zipcode(zipcode)\n",
    "        county = zipcode_data.county\n",
    "        return county\n",
    "\n",
    "    # apply the function to create a new column \"County\"\n",
    "    df_reg['County'] = df_reg['ZIP Code'].apply(zipcode_to_county)\n",
    "\n",
    "    nan_rows = df_reg[df_reg.isna().any(axis=1)]\n",
    "\n",
    "    df_reg = df_reg.groupby([\"State\", \"Registration Date\", \"Drivetrain Type\", \"County\"]).agg('sum').drop(columns = [\"ZIP Code\"]).reset_index()\n",
    "\n",
    "    start_date = pd.to_datetime('2017-01-01')\n",
    "    end_date = pd.to_datetime('2021-12-31')\n",
    "    df_reg = df_reg[(df_reg['Registration Date'] >= start_date) & (df_reg['Registration Date'] <= end_date)]\n",
    "\n",
    "    df_reg[df_reg['County'] == \"\"]\n",
    "\n",
    "    # merge ev data in main df\n",
    "    df_reg_ev = pd.merge(df_reg, df_ev, left_on='Registration Date', right_on='Month', how='left')\n",
    "    df_reg_ev = df_reg_ev.drop(['Month'], axis=1)\n",
    "\n",
    "    # Since we don't have ev price data for earlier dates, set all NaN to price for 2020-01-01\n",
    "    fill_val = {'Average EV Price' : df_ev['Average EV Price'][0], 'New Car Average' : df_ev['New Car Average'][0]}\n",
    "    df_reg_ev = df_reg_ev.fillna(value=fill_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        State  ZIP Code Registration Date Drivetrain Type  Vehicle Count  \\\n",
      "0         TX     75001        2019-11-01            PHEV              2   \n",
      "1         TX     75001        2020-01-01            PHEV              9   \n",
      "2         TX     75001        2020-02-01             BEV              4   \n",
      "3         TX     75001        2020-04-01             BEV              6   \n",
      "4         TX     75001        2020-05-01            PHEV             16   \n",
      "...      ...       ...               ...             ...            ...   \n",
      "229929    NY     14905        2022-12-01            PHEV              6   \n",
      "229930    NY     14905        2023-01-01             BEV              5   \n",
      "229931    NY     14905        2023-01-01            PHEV              6   \n",
      "229932    NY     14905        2023-02-01             BEV              1   \n",
      "229933    NY     14905        2023-02-01            PHEV              2   \n",
      "\n",
      "       Average EV Price New Car Average  \n",
      "0              $54,669         $38,747   \n",
      "1              $54,669         $38,747   \n",
      "2              $56,326         $38,550   \n",
      "3              $57,757         $39,904   \n",
      "4              $58,863         $39,138   \n",
      "...                 ...             ...  \n",
      "229929         $61,448         $49,507   \n",
      "229930         $58,725         $49,388   \n",
      "229931         $58,725         $49,388   \n",
      "229932         $58,385         $48,763   \n",
      "229933         $58,385         $48,763   \n",
      "\n",
      "[229934 rows x 7 columns]>\n",
      "Missing census data in:\n",
      "zip codes =  []\n",
      "states =  []\n",
      "Total num of zips =  0\n",
      "Total entries w/ nan =  0\n",
      "Total entries in df =  229934\n",
      "Download /Users/donokoye/.uszipcode/simple_db.sqlite from https://github.com/MacHu-GWU/uszipcode-project/releases/download/1.0.1.db/simple_db.sqlite ...\n",
      "  1.00 MB downloaded ...\n",
      "  2.00 MB downloaded ...\n",
      "  3.00 MB downloaded ...\n",
      "  4.00 MB downloaded ...\n",
      "  5.00 MB downloaded ...\n",
      "  6.00 MB downloaded ...\n",
      "  7.00 MB downloaded ...\n",
      "  8.00 MB downloaded ...\n",
      "  9.00 MB downloaded ...\n",
      "  10.00 MB downloaded ...\n",
      "  11.00 MB downloaded ...\n",
      "  Complete!\n",
      "State                        object\n",
      "ZIP Code                      int64\n",
      "Registration Date    datetime64[ns]\n",
      "Drivetrain Type              object\n",
      "Vehicle Count                 int64\n",
      "Average EV Price             object\n",
      "New Car Average              object\n",
      "County                       object\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['household_income', 'population'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000018?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000018?line=1'>2</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb Cell 5'\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mdtypes)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=59'>60</a>\u001b[0m \u001b[39m# group the dataframe by State, Registration Date, Drivetrain Type, and County\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=60'>61</a>\u001b[0m grouped_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mState\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mRegistration Date\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mDrivetrain Type\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCounty\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49magg({\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=61'>62</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mVehicle Count\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=62'>63</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mAverage EV Price\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=63'>64</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mNew Car Average\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=64'>65</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mpopulation\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=65'>66</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mhousehold_income\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=66'>67</a>\u001b[0m })\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=68'>69</a>\u001b[0m \u001b[39m# reset the index to turn the groupby result into a dataframe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000002?line=69'>70</a>\u001b[0m grouped_df \u001b[39m=\u001b[39m grouped_df\u001b[39m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/groupby/generic.py:869\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[1;32m    868\u001b[0m op \u001b[39m=\u001b[39m GroupByApply(\u001b[39mself\u001b[39m, func, args, kwargs)\n\u001b[0;32m--> 869\u001b[0m result \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49magg()\n\u001b[1;32m    870\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dict_like(func) \u001b[39mand\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:168\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m is_dict_like(arg):\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magg_dict_like()\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(arg):\n\u001b[1;32m    170\u001b[0m     \u001b[39m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:473\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m     selected_obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_selected_obj\n\u001b[1;32m    471\u001b[0m     selection \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_selection\n\u001b[0;32m--> 473\u001b[0m arg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize_dictlike_arg(\u001b[39m\"\u001b[39;49m\u001b[39magg\u001b[39;49m\u001b[39m\"\u001b[39;49m, selected_obj, arg)\n\u001b[1;32m    475\u001b[0m \u001b[39mif\u001b[39;00m selected_obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    476\u001b[0m     \u001b[39m# key only used for output\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     colg \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_gotitem(selection, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:591\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cols) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    590\u001b[0m         cols_sorted \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(safe_sort(\u001b[39mlist\u001b[39m(cols)))\n\u001b[0;32m--> 591\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn(s) \u001b[39m\u001b[39m{\u001b[39;00mcols_sorted\u001b[39m}\u001b[39;00m\u001b[39m do not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    593\u001b[0m is_aggregator \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39misinstance\u001b[39m(x, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mdict\u001b[39m))\n\u001b[1;32m    595\u001b[0m \u001b[39m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[39m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[39m# be list-likes\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['household_income', 'population'] do not exist\""
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
