{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Class and Training Functions\n",
    "Define Class and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nn_model, X_train, y_train, X_eval, y_eval, config, max_iter=50, batch_size=32, print_n=10, verbose=True):\n",
    "    '''\n",
    "    Trains neural network model on X_train, y_train data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.array\n",
    "        matrix of training data features\n",
    "    y_train: np.array\n",
    "        vector of training data labels\n",
    "    max_iter: int\n",
    "        maximum number of iterations to train for\n",
    "    batch_size: int\n",
    "        batch size to use when training w/ SGD\n",
    "    print_n: int\n",
    "        print training progress every print_n steps\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    nn_model: torch.nn.Module\n",
    "        trained neural network model\n",
    "    '''\n",
    "    # convert to tensors (for Pytorch)\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    y_train_tensor = torch.tensor(y_train)\n",
    "    X_test_tensor = torch.tensor(X_eval)\n",
    "    y_test_tensor = torch.tensor(y_eval)\n",
    "    # intialize neural network\n",
    "    n_samples, n_features = X_train_tensor.shape\n",
    "    #nn_model = NN(n_features, k)\n",
    "    nn_model.train()  # put model in train mode\n",
    "    # initialize mse loss function\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    # train with (mini-batch) SGD; initialize optimizer\n",
    "    #opt = torch.optim.SGD(nn_model.parameters(), lr=1e-4)\n",
    "    opt = torch.optim.SGD(nn_model.parameters(), lr=config['lr'],  momentum=0.9)\n",
    "    losses_test = []\n",
    "    for it in range(max_iter):\n",
    "        # save losses across all batches\n",
    "        losses = []\n",
    "        # loop through data in batches\n",
    "        for batch_start in range(0, n_samples, batch_size):\n",
    "            # reset gradients to zero\n",
    "            opt.zero_grad()\n",
    "            # form batch\n",
    "            X_batch = X_train_tensor[batch_start:batch_start+batch_size]\n",
    "            y_batch = y_train_tensor[batch_start:batch_start+batch_size]\n",
    "            X_batch_test = X_test_tensor[batch_start:batch_start+batch_size]\n",
    "            y_batch_test = y_test_tensor[batch_start:batch_start+batch_size]\n",
    "            # pass batch through neural net to get prediction\n",
    "            y_pred = nn_model(X_batch.float())\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "            y_pred_test = nn_model(X_batch_test.float())\n",
    "            y_pred_test = y_pred_test.unsqueeze(1)\n",
    "            #print(y_pred)\n",
    "            # compute MSE loss\n",
    "            loss = mse_loss(y_pred, y_batch[:, None].float())\n",
    "            loss_test = mse_loss(y_pred_test, y_batch_test[:, None].float())\n",
    "            # back-propagate loss\n",
    "            loss.backward()\n",
    "            # update model parameters based on backpropogated gradients\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "            losses_test.append(loss.item())\n",
    "        if verbose and it % print_n == 0:\n",
    "            print(f\"Mean Train MSE at step {it}: {np.mean(losses)}\")\n",
    "\n",
    "        with tune.checkpoint_dir(it) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "            torch.save((nn_model.state_dict(), opt.state_dict()), path)\n",
    "        tune.report(\n",
    "            loss = loss.item()\n",
    "            #loss=valid_epoch_loss, accuracy=valid_epoch_acc\n",
    "        )\n",
    "\n",
    "    return nn_model, losses_test\n",
    "\n",
    "def evaluate_model(nn_model, X_eval, y_eval, batch_size=32):\n",
    "    '''\n",
    "    Evaluates trained neural network model on X_eval, y_eval data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_model: torch.nn.Module\n",
    "        trained neural network model\n",
    "    X_eval: np.array\n",
    "        matrix of training data features\n",
    "    y_eval: np.array\n",
    "        vector of training data labels\n",
    "    batch_size: int\n",
    "        batch size to looping over dataset to generate predictions\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse: float\n",
    "        MSE of trained model on X_eval, y_eval data\n",
    "    '''\n",
    "    # initialize mse loss function\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    # convert to tensors (for Pytorch)\n",
    "    X_eval_tensor = torch.tensor(X_eval)\n",
    "    y_eval_tensor = torch.tensor(y_eval)\n",
    "    n_samples = X_eval_tensor.shape[0]\n",
    "    nn_model.eval() # put in eval mode\n",
    "    # loop over data and generate predictions\n",
    "    preds = []\n",
    "    for batch_start in range(0, n_samples, batch_size):\n",
    "        # form batch\n",
    "        X_batch = X_eval_tensor[batch_start:batch_start+batch_size]\n",
    "        y_batch = y_eval_tensor[batch_start:batch_start+batch_size]\n",
    "        with torch.no_grad():  # no need to compute gradients during evaluation\n",
    "            # pass batch through neural net to get prediction\n",
    "            y_pred = nn_model(X_batch.float())\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "            preds.append(y_pred)\n",
    "    # compute MSE across all samples\n",
    "    all_preds = torch.cat(preds)\n",
    "    loss = mse_loss(all_preds, y_eval_tensor[:, None].float()).item()\n",
    "    return loss\n",
    "\n",
    "class NN(nn.Module):\n",
    "    '''\n",
    "    Class for fully connected neural net.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            input dimension (i.e., # of features in each example passed to the network)\n",
    "        hidden_dim: int\n",
    "            number of nodes in hidden layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            # Network has a single hidden layer\n",
    "            # Apply ReLU activation in between the hidden layer and output node\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NN_configureable(nn.Module):\n",
    "    '''\n",
    "    Class for fully connected neural net.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim=1, hidden_layers=1):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            input dimension (i.e., # of features in each example passed to the network)\n",
    "        hidden_dim: int\n",
    "            number of nodes in hidden layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.layers = nn.ModuleDict()\n",
    "        print(hidden_dim)\n",
    "        # Define input layer\n",
    "        self.layers[\"input\"] = nn.Linear(in_features = input_dim, out_features = hidden_dim)\n",
    "        # Define hidden layers\n",
    "        for i in range(self.hidden_layers):\n",
    "            self.layers[f\"hidden_{i}\"] = nn.Linear(in_features = hidden_dim, out_features = hidden_dim)\n",
    "        # Define output layer\n",
    "        self.layers[\"output\"] = nn.Linear(in_features = hidden_dim, out_features = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[\"input\"](x)\n",
    "        for i in range(self.hidden_layers):\n",
    "            x = F.relu(self.layers[f\"hidden_{i}\"](x))\n",
    "\n",
    "        return self.layers[\"output\"](x)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Function for Ray Tune\n",
    "Hyperparameter search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "Import combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():   \n",
    "    # Import data\n",
    "    X = pd.read_csv('./Data/df_X_county.csv')\n",
    "    X['constant'] = 1\n",
    "    y = pd.read_csv('./Data/df_y_county.csv')\n",
    "\n",
    "    # check if any nan values\n",
    "    nan_row_X = X[X.isna().any(axis=1)]\n",
    "    #print(nan_row_X)\n",
    "    nan_row_y = y[y.isna().any(axis=1)]\n",
    "    #print(nan_row_y)\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    # split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # standardize X\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    #print(X_test.shape)\n",
    "    #print(y_test.shape)\n",
    "    \n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    # train NN model to predict EV registration using train data\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    n_samples, input_dim = X_train_tensor.shape\n",
    "\n",
    "    # Search Function for Ray Tune - Hyperparameter search\n",
    "    \n",
    "    # Define the parameter search configuration.\n",
    "    config = {\n",
    "        \"n_layers\": \n",
    "            tune.sample_from(lambda _: 2 ** np.random.randint(1, 5)),\n",
    "        \"n_hidden_dim\": \n",
    "            tune.sample_from(lambda _: 2 ** np.random.randint(4, 8)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "\n",
    "    max_num_iter = 50\n",
    "    grace_period = 1\n",
    "    # Number of Ray Tune random search experiments to run.\n",
    "    num_samples = 20\n",
    "    \n",
    "    # Schduler to stop bad performing trails.\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t = max_num_iter,\n",
    "        grace_period = grace_period,\n",
    "        reduction_factor = 2 \n",
    "    )\n",
    "\n",
    "    # Reporter to show on command line/output window\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "\n",
    "    nn_model_place = NN_configureable(input_dim, config['n_hidden_dim'], config['n_layers'])\n",
    "\n",
    "    # Start Ray Tune search\n",
    "    result = tune.run(\n",
    "        train_model(nn_model_place, X_train, y_train, X_test, y_test),\n",
    "        resources_per_trial = {\"cpu\": CPU, \"gpu\": GPU},\n",
    "        config = config,\n",
    "        num_samples = num_samples,\n",
    "        scheduler = scheduler,\n",
    "        local_dir = '../outputs/raytune_result',\n",
    "        keep_checkpoints_num = 1,\n",
    "        checkpoint_score_attr = 'min-validation_loss',\n",
    "        progress_reporter = reporter\n",
    "    )\n",
    "\n",
    "    # Extract the best trial run from the search.\n",
    "    best_trial = result.get_best_trial(\n",
    "        'loss', 'min', 'last'\n",
    "    )\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation acc: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    \n",
    "    #n_layers = np.arange(1, 5) # iterate through hidden layer count\n",
    "    #n_hidden_dim = np.arange(8, 65, 8)  # iterate through hidden layer node count\n",
    "    mse_dict = {}\n",
    "    \n",
    "    #for i in range(len(n_layers)):\n",
    "    #    for j in range(len(n_hidden_dim)):\n",
    "    #        tuple_place = (i, j)\n",
    "    #        nn_model_place = NN_configureable(input_dim, hidden_dim = j, hidden_layers = i)\n",
    "    #        nn_model_result = train_model(nn_model_place, X_train, y_train, X_test, y_test, 32)\n",
    "    #        train_mse =  evaluate_model(nn_model_result[0], X_train, y_train)\n",
    "    #        test_mse = evaluate_model(nn_model_result[0], X_test, y_test)\n",
    "    #        #print(tuple_place)\n",
    "    #        train_test_list = [train_mse, test_mse]\n",
    "    #        mse_dict[tuple_place] = train_test_list\n",
    "            \n",
    "        \n",
    "        \n",
    "    #nn_model_place = NN_configureable(input_dim, 8, 2)\n",
    "    #nn_model_place = NN(input_dim, 8)\n",
    "    #nn_model_result = train_model(nn_model_place, X_train, y_train, X_test, y_test, 32)\n",
    "    #train_mse = evaluate_model(nn_model_result[0], X_train, y_train)\n",
    "    #test_mse = evaluate_model(nn_model_result[0], X_test, y_test)\n",
    "        \n",
    "    for key in mse_dict:\n",
    "        print(f\"Train MSE for model: hidden_layers = {key[0]}, hidden_dim = {key[1]} is: {mse_dict[key][0]}\")\n",
    "        print(f\"Test MSE for model: hidden_layers = {key[0]}, hidden_dim = {key[1]} is: {mse_dict[key][1]}\")\n",
    "\n",
    "    # plot the model's test errors\n",
    "    #plt.plot(range(len(nn_model_result[1])), nn_model_result[1])\n",
    "    # axis labels\n",
    "    plt.xlabel('Iteration Step')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.title(\"Model 1 - Hidden Layer - ReLU\")\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.search.sample.Function object at 0x17cc3d8d0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000007?line=1'>2</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb Cell 6'\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=58'>59</a>\u001b[0m \u001b[39m# Reporter to show on command line/output window\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=59'>60</a>\u001b[0m reporter \u001b[39m=\u001b[39m CLIReporter(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=60'>61</a>\u001b[0m     metric_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtraining_iteration\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=62'>63</a>\u001b[0m nn_model_place \u001b[39m=\u001b[39m NN_configureable(input_dim, config[\u001b[39m'\u001b[39;49m\u001b[39mn_hidden_dim\u001b[39;49m\u001b[39m'\u001b[39;49m], config[\u001b[39m'\u001b[39;49m\u001b[39mn_layers\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=64'>65</a>\u001b[0m \u001b[39m# Start Ray Tune search\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=65'>66</a>\u001b[0m result \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mrun(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=66'>67</a>\u001b[0m     train_model(nn_model_place, X_train, y_train, X_test, y_test),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=67'>68</a>\u001b[0m     resources_per_trial \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m: CPU, \u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m: GPU},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=74'>75</a>\u001b[0m     progress_reporter \u001b[39m=\u001b[39m reporter\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000006?line=75'>76</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb Cell 3'\u001b[0m in \u001b[0;36mNN_configureable.__init__\u001b[0;34m(self, input_dim, hidden_dim, hidden_layers)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000003?line=167'>168</a>\u001b[0m \u001b[39mprint\u001b[39m(hidden_dim)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000003?line=168'>169</a>\u001b[0m \u001b[39m# Define input layer\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000003?line=169'>170</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(in_features \u001b[39m=\u001b[39;49m input_dim, out_features \u001b[39m=\u001b[39;49m hidden_dim)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000003?line=170'>171</a>\u001b[0m \u001b[39m# Define hidden layers\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/donokoye/Documents/Spring_23/ML_1.C51/Final_Project/ev_adoption_ml/neural_net.ipynb#ch0000003?line=171'>172</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layers):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
