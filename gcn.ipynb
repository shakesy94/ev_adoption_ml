{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1f20af8-74e6-40a0-9e2c-42906f1613db",
   "metadata": {},
   "source": [
    "# GNN \n",
    "\n",
    "\n",
    "Code based on \n",
    "\n",
    "https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=zF5bw3m9UrMy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a971f309-cb9a-45f2-b697-74d55f2e3bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import random\n",
    "from math import floor\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4f79c5f-51de-4652-8cac-01edefedc990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1250])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "X = pd.read_csv('./Data/df_X_gnn.csv')\n",
    "y = pd.read_csv('./Data/df_y_gnn.csv')\n",
    "edge_idx = pd.read_csv('./Data/df_e_idx_gnn.csv')\n",
    "\n",
    "edge_att = pd.read_csv('./Data/df_e_att_gnn.csv')\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "y = y[:,0]\n",
    "\n",
    "\n",
    "\n",
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).float()\n",
    "edge_idx = torch.tensor(edge_idx.to_numpy()).int()\n",
    "edge_att = torch.tensor(edge_att.to_numpy()).float()\n",
    "\n",
    "#edge_idx = edge_idx.transpose(0,1)\n",
    "\n",
    "edge_idx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f466c9-6b24-407d-a1f8-b2a0004974d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make masks\n",
    "n = X.shape[0]\n",
    "randomassort = list(range(n))\n",
    "random.shuffle(randomassort)\n",
    "\n",
    "# percentage of training mask\n",
    "train_perc = 0.1\n",
    "max_train = floor(len(randomassort) * train_perc)\n",
    "train_mask_idx = torch.tensor(randomassort[:max_train])\n",
    "test_mask_idx = torch.tensor(randomassort[max_train:])\n",
    "train_mask = torch.zeros(n); test_mask = torch.zeros(n)\n",
    "train_mask.scatter_(0, train_mask_idx, 1)\n",
    "test_mask.scatter_(0, test_mask_idx, 1)\n",
    "train_mask = train_mask.type(torch.bool)\n",
    "test_mask = test_mask.type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6e0485-3bbd-4f3a-9b6b-5551218b185b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[297, 10], edge_index=[2, 1250], y=[297], test_mask=[297], train_mask=[297])\n",
      "==============================================================\n",
      "Number of nodes: 297\n",
      "Number of edges: 1250\n",
      "Average node degree: 4.21\n",
      "Number of training nodes: 29\n",
      "Training node label rate: 0.10\n",
      "Has isolated nodes: True\n"
     ]
    }
   ],
   "source": [
    "data = Data(x = X, y = y, edge_index = edge_idx)#, edge_attr = edge_att)\n",
    "\n",
    "data.test_mask = test_mask\n",
    "data.train_mask = train_mask\n",
    "print(data)\n",
    "print('==============================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3279d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6059e+04, 4.1172e+04, 5.0984e+05,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [5.6059e+04, 4.1172e+04, 6.4998e+05,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [5.6059e+04, 4.1172e+04, 1.3588e+04,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [5.6059e+04, 4.1172e+04, 4.1386e+04,  ..., 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [5.6059e+04, 4.1172e+04, 4.9018e+04,  ..., 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [5.6059e+04, 4.1172e+04, 3.5275e+04,  ..., 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5691920a-d1b3-4834-ac10-7a0573119d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, with_labels=False)#, pos=nx.spring_layout(G, seed=42)),\n",
    "                     #node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def visualize_embedding(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    h = h.detach().cpu().numpy()\n",
    "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "    if epoch is not None and loss is not None:\n",
    "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "visualize_graph(G, color=data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "003ed154-b439-4cec-afc4-146a83061332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(10, 4)\n",
      "  (conv2): GCNConv(4, 4)\n",
      "  (conv3): GCNConv(4, 2)\n",
      "  (pred): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GCNConv(data.num_features, 4)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.pred = Linear(2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.pred(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7da0b4f4-7a36-452a-98b4-eb7bb946041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(3442861., grad_fn=<MseLossBackward0>)\n",
      "tensor(3442861., grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(3442851.2500, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(3442841.7500, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(3442832., grad_fn=<MseLossBackward0>)\n",
      "4 tensor(3442822.2500, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(3442812.5000, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(3442803., grad_fn=<MseLossBackward0>)\n",
      "7 tensor(3442793., grad_fn=<MseLossBackward0>)\n",
      "8 tensor(3442783.2500, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(3442773.7500, grad_fn=<MseLossBackward0>)\n",
      "10 tensor(3442764., grad_fn=<MseLossBackward0>)\n",
      "tensor(3442764., grad_fn=<MseLossBackward0>)\n",
      "11 tensor(3442754., grad_fn=<MseLossBackward0>)\n",
      "12 tensor(3442744.5000, grad_fn=<MseLossBackward0>)\n",
      "13 tensor(3442735., grad_fn=<MseLossBackward0>)\n",
      "14 tensor(3442725.2500, grad_fn=<MseLossBackward0>)\n",
      "15 tensor(3442715.5000, grad_fn=<MseLossBackward0>)\n",
      "16 tensor(3442705.7500, grad_fn=<MseLossBackward0>)\n",
      "17 tensor(3442696., grad_fn=<MseLossBackward0>)\n",
      "18 tensor(3442686.2500, grad_fn=<MseLossBackward0>)\n",
      "19 tensor(3442676.5000, grad_fn=<MseLossBackward0>)\n",
      "20 tensor(3442666.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3442666.5000, grad_fn=<MseLossBackward0>)\n",
      "21 tensor(3442657., grad_fn=<MseLossBackward0>)\n",
      "22 tensor(3442647.2500, grad_fn=<MseLossBackward0>)\n",
      "23 tensor(3442637.5000, grad_fn=<MseLossBackward0>)\n",
      "24 tensor(3442628., grad_fn=<MseLossBackward0>)\n",
      "25 tensor(3442618.2500, grad_fn=<MseLossBackward0>)\n",
      "26 tensor(3442608.7500, grad_fn=<MseLossBackward0>)\n",
      "27 tensor(3442599., grad_fn=<MseLossBackward0>)\n",
      "28 tensor(3442589.2500, grad_fn=<MseLossBackward0>)\n",
      "29 tensor(3442579.7500, grad_fn=<MseLossBackward0>)\n",
      "30 tensor(3442570.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3442570.5000, grad_fn=<MseLossBackward0>)\n",
      "31 tensor(3442560.7500, grad_fn=<MseLossBackward0>)\n",
      "32 tensor(3442551.7500, grad_fn=<MseLossBackward0>)\n",
      "33 tensor(3442542.2500, grad_fn=<MseLossBackward0>)\n",
      "34 tensor(3442532.7500, grad_fn=<MseLossBackward0>)\n",
      "35 tensor(3442523.2500, grad_fn=<MseLossBackward0>)\n",
      "36 tensor(3442514.2500, grad_fn=<MseLossBackward0>)\n",
      "37 tensor(3442504.7500, grad_fn=<MseLossBackward0>)\n",
      "38 tensor(3442495.5000, grad_fn=<MseLossBackward0>)\n",
      "39 tensor(3442486.2500, grad_fn=<MseLossBackward0>)\n",
      "40 tensor(3442476.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3442476.7500, grad_fn=<MseLossBackward0>)\n",
      "41 tensor(3442467.5000, grad_fn=<MseLossBackward0>)\n",
      "42 tensor(3442458.5000, grad_fn=<MseLossBackward0>)\n",
      "43 tensor(3442449.2500, grad_fn=<MseLossBackward0>)\n",
      "44 tensor(3442440.7500, grad_fn=<MseLossBackward0>)\n",
      "45 tensor(3442431.7500, grad_fn=<MseLossBackward0>)\n",
      "46 tensor(3442422.5000, grad_fn=<MseLossBackward0>)\n",
      "47 tensor(3442413.7500, grad_fn=<MseLossBackward0>)\n",
      "48 tensor(3442405., grad_fn=<MseLossBackward0>)\n",
      "49 tensor(3442395.7500, grad_fn=<MseLossBackward0>)\n",
      "50 tensor(3442387.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3442387.2500, grad_fn=<MseLossBackward0>)\n",
      "51 tensor(3442378.5000, grad_fn=<MseLossBackward0>)\n",
      "52 tensor(3442369.5000, grad_fn=<MseLossBackward0>)\n",
      "53 tensor(3442361.5000, grad_fn=<MseLossBackward0>)\n",
      "54 tensor(3442352.5000, grad_fn=<MseLossBackward0>)\n",
      "55 tensor(3442344., grad_fn=<MseLossBackward0>)\n",
      "56 tensor(3442335.5000, grad_fn=<MseLossBackward0>)\n",
      "57 tensor(3442327.2500, grad_fn=<MseLossBackward0>)\n",
      "58 tensor(3442318.7500, grad_fn=<MseLossBackward0>)\n",
      "59 tensor(3442310.5000, grad_fn=<MseLossBackward0>)\n",
      "60 tensor(3442302.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3442302.2500, grad_fn=<MseLossBackward0>)\n",
      "61 tensor(3442293.7500, grad_fn=<MseLossBackward0>)\n",
      "62 tensor(3442285.5000, grad_fn=<MseLossBackward0>)\n",
      "63 tensor(3442277.2500, grad_fn=<MseLossBackward0>)\n",
      "64 tensor(3442269.2500, grad_fn=<MseLossBackward0>)\n",
      "65 tensor(3442261.2500, grad_fn=<MseLossBackward0>)\n",
      "66 tensor(3442253.2500, grad_fn=<MseLossBackward0>)\n",
      "67 tensor(3442245., grad_fn=<MseLossBackward0>)\n",
      "68 tensor(3442237., grad_fn=<MseLossBackward0>)\n",
      "69 tensor(3442229.2500, grad_fn=<MseLossBackward0>)\n",
      "70 tensor(3442221.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3442221.7500, grad_fn=<MseLossBackward0>)\n",
      "71 tensor(3442213.7500, grad_fn=<MseLossBackward0>)\n",
      "72 tensor(3442205.7500, grad_fn=<MseLossBackward0>)\n",
      "73 tensor(3442198., grad_fn=<MseLossBackward0>)\n",
      "74 tensor(3442190.2500, grad_fn=<MseLossBackward0>)\n",
      "75 tensor(3442182.7500, grad_fn=<MseLossBackward0>)\n",
      "76 tensor(3442174.7500, grad_fn=<MseLossBackward0>)\n",
      "77 tensor(3442167.2500, grad_fn=<MseLossBackward0>)\n",
      "78 tensor(3442159.7500, grad_fn=<MseLossBackward0>)\n",
      "79 tensor(3442152.2500, grad_fn=<MseLossBackward0>)\n",
      "80 tensor(3442144.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3442144.7500, grad_fn=<MseLossBackward0>)\n",
      "81 tensor(3442137., grad_fn=<MseLossBackward0>)\n",
      "82 tensor(3442129.7500, grad_fn=<MseLossBackward0>)\n",
      "83 tensor(3442122.5000, grad_fn=<MseLossBackward0>)\n",
      "84 tensor(3442114.7500, grad_fn=<MseLossBackward0>)\n",
      "85 tensor(3442107.5000, grad_fn=<MseLossBackward0>)\n",
      "86 tensor(3442100.2500, grad_fn=<MseLossBackward0>)\n",
      "87 tensor(3442093., grad_fn=<MseLossBackward0>)\n",
      "88 tensor(3442086., grad_fn=<MseLossBackward0>)\n",
      "89 tensor(3442078.2500, grad_fn=<MseLossBackward0>)\n",
      "90 tensor(3442071., grad_fn=<MseLossBackward0>)\n",
      "tensor(3442071., grad_fn=<MseLossBackward0>)\n",
      "91 tensor(3442064., grad_fn=<MseLossBackward0>)\n",
      "92 tensor(3442057., grad_fn=<MseLossBackward0>)\n",
      "93 tensor(3442050., grad_fn=<MseLossBackward0>)\n",
      "94 tensor(3442042.7500, grad_fn=<MseLossBackward0>)\n",
      "95 tensor(3442035.2500, grad_fn=<MseLossBackward0>)\n",
      "96 tensor(3442028.2500, grad_fn=<MseLossBackward0>)\n",
      "97 tensor(3442021.7500, grad_fn=<MseLossBackward0>)\n",
      "98 tensor(3442014.7500, grad_fn=<MseLossBackward0>)\n",
      "99 tensor(3442007.2500, grad_fn=<MseLossBackward0>)\n",
      "100 tensor(3442000.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3442000.7500, grad_fn=<MseLossBackward0>)\n",
      "101 tensor(3441993.2500, grad_fn=<MseLossBackward0>)\n",
      "102 tensor(3441986.7500, grad_fn=<MseLossBackward0>)\n",
      "103 tensor(3441980., grad_fn=<MseLossBackward0>)\n",
      "104 tensor(3441972.7500, grad_fn=<MseLossBackward0>)\n",
      "105 tensor(3441966., grad_fn=<MseLossBackward0>)\n",
      "106 tensor(3441959.2500, grad_fn=<MseLossBackward0>)\n",
      "107 tensor(3441952.5000, grad_fn=<MseLossBackward0>)\n",
      "108 tensor(3441945.5000, grad_fn=<MseLossBackward0>)\n",
      "109 tensor(3441938.5000, grad_fn=<MseLossBackward0>)\n",
      "110 tensor(3441931.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441931.7500, grad_fn=<MseLossBackward0>)\n",
      "111 tensor(3441924.7500, grad_fn=<MseLossBackward0>)\n",
      "112 tensor(3441918.2500, grad_fn=<MseLossBackward0>)\n",
      "113 tensor(3441911.5000, grad_fn=<MseLossBackward0>)\n",
      "114 tensor(3441904.7500, grad_fn=<MseLossBackward0>)\n",
      "115 tensor(3441897.7500, grad_fn=<MseLossBackward0>)\n",
      "116 tensor(3441891., grad_fn=<MseLossBackward0>)\n",
      "117 tensor(3441884.5000, grad_fn=<MseLossBackward0>)\n",
      "118 tensor(3441877.7500, grad_fn=<MseLossBackward0>)\n",
      "119 tensor(3441871.5000, grad_fn=<MseLossBackward0>)\n",
      "120 tensor(3441865., grad_fn=<MseLossBackward0>)\n",
      "tensor(3441865., grad_fn=<MseLossBackward0>)\n",
      "121 tensor(3441858., grad_fn=<MseLossBackward0>)\n",
      "122 tensor(3441851.5000, grad_fn=<MseLossBackward0>)\n",
      "123 tensor(3441844.7500, grad_fn=<MseLossBackward0>)\n",
      "124 tensor(3441838., grad_fn=<MseLossBackward0>)\n",
      "125 tensor(3441831.5000, grad_fn=<MseLossBackward0>)\n",
      "126 tensor(3441825.2500, grad_fn=<MseLossBackward0>)\n",
      "127 tensor(3441818.7500, grad_fn=<MseLossBackward0>)\n",
      "128 tensor(3441812.2500, grad_fn=<MseLossBackward0>)\n",
      "129 tensor(3441805.2500, grad_fn=<MseLossBackward0>)\n",
      "130 tensor(3441798.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441798.7500, grad_fn=<MseLossBackward0>)\n",
      "131 tensor(3441792.2500, grad_fn=<MseLossBackward0>)\n",
      "132 tensor(3441785.7500, grad_fn=<MseLossBackward0>)\n",
      "133 tensor(3441779.2500, grad_fn=<MseLossBackward0>)\n",
      "134 tensor(3441773., grad_fn=<MseLossBackward0>)\n",
      "135 tensor(3441766., grad_fn=<MseLossBackward0>)\n",
      "136 tensor(3441759.5000, grad_fn=<MseLossBackward0>)\n",
      "137 tensor(3441753., grad_fn=<MseLossBackward0>)\n",
      "138 tensor(3441747., grad_fn=<MseLossBackward0>)\n",
      "139 tensor(3441740.5000, grad_fn=<MseLossBackward0>)\n",
      "140 tensor(3441733.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441733.7500, grad_fn=<MseLossBackward0>)\n",
      "141 tensor(3441727.2500, grad_fn=<MseLossBackward0>)\n",
      "142 tensor(3441721.2500, grad_fn=<MseLossBackward0>)\n",
      "143 tensor(3441714.5000, grad_fn=<MseLossBackward0>)\n",
      "144 tensor(3441708.5000, grad_fn=<MseLossBackward0>)\n",
      "145 tensor(3441701.7500, grad_fn=<MseLossBackward0>)\n",
      "146 tensor(3441695.7500, grad_fn=<MseLossBackward0>)\n",
      "147 tensor(3441689., grad_fn=<MseLossBackward0>)\n",
      "148 tensor(3441682.7500, grad_fn=<MseLossBackward0>)\n",
      "149 tensor(3441676.5000, grad_fn=<MseLossBackward0>)\n",
      "150 tensor(3441669.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441669.5000, grad_fn=<MseLossBackward0>)\n",
      "151 tensor(3441663.7500, grad_fn=<MseLossBackward0>)\n",
      "152 tensor(3441657.2500, grad_fn=<MseLossBackward0>)\n",
      "153 tensor(3441650.5000, grad_fn=<MseLossBackward0>)\n",
      "154 tensor(3441644.2500, grad_fn=<MseLossBackward0>)\n",
      "155 tensor(3441638.5000, grad_fn=<MseLossBackward0>)\n",
      "156 tensor(3441631.7500, grad_fn=<MseLossBackward0>)\n",
      "157 tensor(3441625.2500, grad_fn=<MseLossBackward0>)\n",
      "158 tensor(3441619., grad_fn=<MseLossBackward0>)\n",
      "159 tensor(3441612.7500, grad_fn=<MseLossBackward0>)\n",
      "160 tensor(3441606.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441606.7500, grad_fn=<MseLossBackward0>)\n",
      "161 tensor(3441599.7500, grad_fn=<MseLossBackward0>)\n",
      "162 tensor(3441593.7500, grad_fn=<MseLossBackward0>)\n",
      "163 tensor(3441587.5000, grad_fn=<MseLossBackward0>)\n",
      "164 tensor(3441581., grad_fn=<MseLossBackward0>)\n",
      "165 tensor(3441574.5000, grad_fn=<MseLossBackward0>)\n",
      "166 tensor(3441569., grad_fn=<MseLossBackward0>)\n",
      "167 tensor(3441562.2500, grad_fn=<MseLossBackward0>)\n",
      "168 tensor(3441556.2500, grad_fn=<MseLossBackward0>)\n",
      "169 tensor(3441549.5000, grad_fn=<MseLossBackward0>)\n",
      "170 tensor(3441543.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441543.5000, grad_fn=<MseLossBackward0>)\n",
      "171 tensor(3441537.2500, grad_fn=<MseLossBackward0>)\n",
      "172 tensor(3441531., grad_fn=<MseLossBackward0>)\n",
      "173 tensor(3441525., grad_fn=<MseLossBackward0>)\n",
      "174 tensor(3441518.5000, grad_fn=<MseLossBackward0>)\n",
      "175 tensor(3441512., grad_fn=<MseLossBackward0>)\n",
      "176 tensor(3441505.7500, grad_fn=<MseLossBackward0>)\n",
      "177 tensor(3441500., grad_fn=<MseLossBackward0>)\n",
      "178 tensor(3441493.5000, grad_fn=<MseLossBackward0>)\n",
      "179 tensor(3441487.2500, grad_fn=<MseLossBackward0>)\n",
      "180 tensor(3441481., grad_fn=<MseLossBackward0>)\n",
      "tensor(3441481., grad_fn=<MseLossBackward0>)\n",
      "181 tensor(3441475., grad_fn=<MseLossBackward0>)\n",
      "182 tensor(3441468.5000, grad_fn=<MseLossBackward0>)\n",
      "183 tensor(3441462.5000, grad_fn=<MseLossBackward0>)\n",
      "184 tensor(3441456., grad_fn=<MseLossBackward0>)\n",
      "185 tensor(3441450., grad_fn=<MseLossBackward0>)\n",
      "186 tensor(3441443.5000, grad_fn=<MseLossBackward0>)\n",
      "187 tensor(3441437.7500, grad_fn=<MseLossBackward0>)\n",
      "188 tensor(3441431.5000, grad_fn=<MseLossBackward0>)\n",
      "189 tensor(3441425.2500, grad_fn=<MseLossBackward0>)\n",
      "190 tensor(3441419.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441419.2500, grad_fn=<MseLossBackward0>)\n",
      "191 tensor(3441412.5000, grad_fn=<MseLossBackward0>)\n",
      "192 tensor(3441406.7500, grad_fn=<MseLossBackward0>)\n",
      "193 tensor(3441400.7500, grad_fn=<MseLossBackward0>)\n",
      "194 tensor(3441394.2500, grad_fn=<MseLossBackward0>)\n",
      "195 tensor(3441388.2500, grad_fn=<MseLossBackward0>)\n",
      "196 tensor(3441382.5000, grad_fn=<MseLossBackward0>)\n",
      "197 tensor(3441375.7500, grad_fn=<MseLossBackward0>)\n",
      "198 tensor(3441370., grad_fn=<MseLossBackward0>)\n",
      "199 tensor(3441363.7500, grad_fn=<MseLossBackward0>)\n",
      "200 tensor(3441357.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441357.7500, grad_fn=<MseLossBackward0>)\n",
      "201 tensor(3441351., grad_fn=<MseLossBackward0>)\n",
      "202 tensor(3441345.2500, grad_fn=<MseLossBackward0>)\n",
      "203 tensor(3441339., grad_fn=<MseLossBackward0>)\n",
      "204 tensor(3441332.7500, grad_fn=<MseLossBackward0>)\n",
      "205 tensor(3441327., grad_fn=<MseLossBackward0>)\n",
      "206 tensor(3441321., grad_fn=<MseLossBackward0>)\n",
      "207 tensor(3441315.2500, grad_fn=<MseLossBackward0>)\n",
      "208 tensor(3441308.7500, grad_fn=<MseLossBackward0>)\n",
      "209 tensor(3441302.2500, grad_fn=<MseLossBackward0>)\n",
      "210 tensor(3441296.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441296.5000, grad_fn=<MseLossBackward0>)\n",
      "211 tensor(3441290.2500, grad_fn=<MseLossBackward0>)\n",
      "212 tensor(3441284., grad_fn=<MseLossBackward0>)\n",
      "213 tensor(3441278.2500, grad_fn=<MseLossBackward0>)\n",
      "214 tensor(3441272.2500, grad_fn=<MseLossBackward0>)\n",
      "215 tensor(3441266.2500, grad_fn=<MseLossBackward0>)\n",
      "216 tensor(3441260., grad_fn=<MseLossBackward0>)\n",
      "217 tensor(3441254., grad_fn=<MseLossBackward0>)\n",
      "218 tensor(3441248., grad_fn=<MseLossBackward0>)\n",
      "219 tensor(3441241.7500, grad_fn=<MseLossBackward0>)\n",
      "220 tensor(3441235.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441235.5000, grad_fn=<MseLossBackward0>)\n",
      "221 tensor(3441230., grad_fn=<MseLossBackward0>)\n",
      "222 tensor(3441223.5000, grad_fn=<MseLossBackward0>)\n",
      "223 tensor(3441217.7500, grad_fn=<MseLossBackward0>)\n",
      "224 tensor(3441211.2500, grad_fn=<MseLossBackward0>)\n",
      "225 tensor(3441205.5000, grad_fn=<MseLossBackward0>)\n",
      "226 tensor(3441199.5000, grad_fn=<MseLossBackward0>)\n",
      "227 tensor(3441193.5000, grad_fn=<MseLossBackward0>)\n",
      "228 tensor(3441188., grad_fn=<MseLossBackward0>)\n",
      "229 tensor(3441181.5000, grad_fn=<MseLossBackward0>)\n",
      "230 tensor(3441175.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441175.5000, grad_fn=<MseLossBackward0>)\n",
      "231 tensor(3441169.7500, grad_fn=<MseLossBackward0>)\n",
      "232 tensor(3441163.2500, grad_fn=<MseLossBackward0>)\n",
      "233 tensor(3441157.5000, grad_fn=<MseLossBackward0>)\n",
      "234 tensor(3441151., grad_fn=<MseLossBackward0>)\n",
      "235 tensor(3441145.2500, grad_fn=<MseLossBackward0>)\n",
      "236 tensor(3441139.5000, grad_fn=<MseLossBackward0>)\n",
      "237 tensor(3441133.5000, grad_fn=<MseLossBackward0>)\n",
      "238 tensor(3441127.7500, grad_fn=<MseLossBackward0>)\n",
      "239 tensor(3441121.5000, grad_fn=<MseLossBackward0>)\n",
      "240 tensor(3441115.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441115.7500, grad_fn=<MseLossBackward0>)\n",
      "241 tensor(3441109.7500, grad_fn=<MseLossBackward0>)\n",
      "242 tensor(3441103.5000, grad_fn=<MseLossBackward0>)\n",
      "243 tensor(3441097.2500, grad_fn=<MseLossBackward0>)\n",
      "244 tensor(3441091.7500, grad_fn=<MseLossBackward0>)\n",
      "245 tensor(3441085.7500, grad_fn=<MseLossBackward0>)\n",
      "246 tensor(3441079.7500, grad_fn=<MseLossBackward0>)\n",
      "247 tensor(3441073.7500, grad_fn=<MseLossBackward0>)\n",
      "248 tensor(3441068., grad_fn=<MseLossBackward0>)\n",
      "249 tensor(3441062., grad_fn=<MseLossBackward0>)\n",
      "250 tensor(3441055.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3441055.7500, grad_fn=<MseLossBackward0>)\n",
      "251 tensor(3441050.2500, grad_fn=<MseLossBackward0>)\n",
      "252 tensor(3441044., grad_fn=<MseLossBackward0>)\n",
      "253 tensor(3441038.2500, grad_fn=<MseLossBackward0>)\n",
      "254 tensor(3441032.2500, grad_fn=<MseLossBackward0>)\n",
      "255 tensor(3441026.2500, grad_fn=<MseLossBackward0>)\n",
      "256 tensor(3441020.5000, grad_fn=<MseLossBackward0>)\n",
      "257 tensor(3441014.5000, grad_fn=<MseLossBackward0>)\n",
      "258 tensor(3441008.7500, grad_fn=<MseLossBackward0>)\n",
      "259 tensor(3441002.5000, grad_fn=<MseLossBackward0>)\n",
      "260 tensor(3440996.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440996.7500, grad_fn=<MseLossBackward0>)\n",
      "261 tensor(3440991., grad_fn=<MseLossBackward0>)\n",
      "262 tensor(3440985., grad_fn=<MseLossBackward0>)\n",
      "263 tensor(3440979., grad_fn=<MseLossBackward0>)\n",
      "264 tensor(3440973.2500, grad_fn=<MseLossBackward0>)\n",
      "265 tensor(3440967.2500, grad_fn=<MseLossBackward0>)\n",
      "266 tensor(3440961.5000, grad_fn=<MseLossBackward0>)\n",
      "267 tensor(3440955.7500, grad_fn=<MseLossBackward0>)\n",
      "268 tensor(3440950., grad_fn=<MseLossBackward0>)\n",
      "269 tensor(3440943.7500, grad_fn=<MseLossBackward0>)\n",
      "270 tensor(3440937.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440937.7500, grad_fn=<MseLossBackward0>)\n",
      "271 tensor(3440932.2500, grad_fn=<MseLossBackward0>)\n",
      "272 tensor(3440926.2500, grad_fn=<MseLossBackward0>)\n",
      "273 tensor(3440920.2500, grad_fn=<MseLossBackward0>)\n",
      "274 tensor(3440914.2500, grad_fn=<MseLossBackward0>)\n",
      "275 tensor(3440909., grad_fn=<MseLossBackward0>)\n",
      "276 tensor(3440902.7500, grad_fn=<MseLossBackward0>)\n",
      "277 tensor(3440897., grad_fn=<MseLossBackward0>)\n",
      "278 tensor(3440890.7500, grad_fn=<MseLossBackward0>)\n",
      "279 tensor(3440884.7500, grad_fn=<MseLossBackward0>)\n",
      "280 tensor(3440879.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440879.2500, grad_fn=<MseLossBackward0>)\n",
      "281 tensor(3440873.7500, grad_fn=<MseLossBackward0>)\n",
      "282 tensor(3440867.7500, grad_fn=<MseLossBackward0>)\n",
      "283 tensor(3440861.5000, grad_fn=<MseLossBackward0>)\n",
      "284 tensor(3440855.7500, grad_fn=<MseLossBackward0>)\n",
      "285 tensor(3440850., grad_fn=<MseLossBackward0>)\n",
      "286 tensor(3440844.2500, grad_fn=<MseLossBackward0>)\n",
      "287 tensor(3440838.2500, grad_fn=<MseLossBackward0>)\n",
      "288 tensor(3440832.7500, grad_fn=<MseLossBackward0>)\n",
      "289 tensor(3440826.5000, grad_fn=<MseLossBackward0>)\n",
      "290 tensor(3440821., grad_fn=<MseLossBackward0>)\n",
      "tensor(3440821., grad_fn=<MseLossBackward0>)\n",
      "291 tensor(3440815., grad_fn=<MseLossBackward0>)\n",
      "292 tensor(3440809.5000, grad_fn=<MseLossBackward0>)\n",
      "293 tensor(3440803.7500, grad_fn=<MseLossBackward0>)\n",
      "294 tensor(3440798.2500, grad_fn=<MseLossBackward0>)\n",
      "295 tensor(3440792., grad_fn=<MseLossBackward0>)\n",
      "296 tensor(3440786., grad_fn=<MseLossBackward0>)\n",
      "297 tensor(3440780.2500, grad_fn=<MseLossBackward0>)\n",
      "298 tensor(3440774.7500, grad_fn=<MseLossBackward0>)\n",
      "299 tensor(3440769., grad_fn=<MseLossBackward0>)\n",
      "300 tensor(3440763.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440763.2500, grad_fn=<MseLossBackward0>)\n",
      "301 tensor(3440757.5000, grad_fn=<MseLossBackward0>)\n",
      "302 tensor(3440752., grad_fn=<MseLossBackward0>)\n",
      "303 tensor(3440746., grad_fn=<MseLossBackward0>)\n",
      "304 tensor(3440739.7500, grad_fn=<MseLossBackward0>)\n",
      "305 tensor(3440734.2500, grad_fn=<MseLossBackward0>)\n",
      "306 tensor(3440728.2500, grad_fn=<MseLossBackward0>)\n",
      "307 tensor(3440722.7500, grad_fn=<MseLossBackward0>)\n",
      "308 tensor(3440717.2500, grad_fn=<MseLossBackward0>)\n",
      "309 tensor(3440711.2500, grad_fn=<MseLossBackward0>)\n",
      "310 tensor(3440705.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440705.7500, grad_fn=<MseLossBackward0>)\n",
      "311 tensor(3440700., grad_fn=<MseLossBackward0>)\n",
      "312 tensor(3440694.2500, grad_fn=<MseLossBackward0>)\n",
      "313 tensor(3440688.5000, grad_fn=<MseLossBackward0>)\n",
      "314 tensor(3440682.5000, grad_fn=<MseLossBackward0>)\n",
      "315 tensor(3440676.7500, grad_fn=<MseLossBackward0>)\n",
      "316 tensor(3440671.2500, grad_fn=<MseLossBackward0>)\n",
      "317 tensor(3440665.5000, grad_fn=<MseLossBackward0>)\n",
      "318 tensor(3440659.7500, grad_fn=<MseLossBackward0>)\n",
      "319 tensor(3440654., grad_fn=<MseLossBackward0>)\n",
      "320 tensor(3440648.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440648.2500, grad_fn=<MseLossBackward0>)\n",
      "321 tensor(3440642.5000, grad_fn=<MseLossBackward0>)\n",
      "322 tensor(3440637.2500, grad_fn=<MseLossBackward0>)\n",
      "323 tensor(3440631.2500, grad_fn=<MseLossBackward0>)\n",
      "324 tensor(3440625.7500, grad_fn=<MseLossBackward0>)\n",
      "325 tensor(3440619.5000, grad_fn=<MseLossBackward0>)\n",
      "326 tensor(3440613.7500, grad_fn=<MseLossBackward0>)\n",
      "327 tensor(3440608.2500, grad_fn=<MseLossBackward0>)\n",
      "328 tensor(3440602.7500, grad_fn=<MseLossBackward0>)\n",
      "329 tensor(3440597.5000, grad_fn=<MseLossBackward0>)\n",
      "330 tensor(3440591.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440591.2500, grad_fn=<MseLossBackward0>)\n",
      "331 tensor(3440585.7500, grad_fn=<MseLossBackward0>)\n",
      "332 tensor(3440580., grad_fn=<MseLossBackward0>)\n",
      "333 tensor(3440574.5000, grad_fn=<MseLossBackward0>)\n",
      "334 tensor(3440568.7500, grad_fn=<MseLossBackward0>)\n",
      "335 tensor(3440563., grad_fn=<MseLossBackward0>)\n",
      "336 tensor(3440557., grad_fn=<MseLossBackward0>)\n",
      "337 tensor(3440551.7500, grad_fn=<MseLossBackward0>)\n",
      "338 tensor(3440546., grad_fn=<MseLossBackward0>)\n",
      "339 tensor(3440540.5000, grad_fn=<MseLossBackward0>)\n",
      "340 tensor(3440534.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440534.7500, grad_fn=<MseLossBackward0>)\n",
      "341 tensor(3440529.5000, grad_fn=<MseLossBackward0>)\n",
      "342 tensor(3440523.5000, grad_fn=<MseLossBackward0>)\n",
      "343 tensor(3440517.7500, grad_fn=<MseLossBackward0>)\n",
      "344 tensor(3440512.5000, grad_fn=<MseLossBackward0>)\n",
      "345 tensor(3440506.7500, grad_fn=<MseLossBackward0>)\n",
      "346 tensor(3440501.2500, grad_fn=<MseLossBackward0>)\n",
      "347 tensor(3440495., grad_fn=<MseLossBackward0>)\n",
      "348 tensor(3440489.7500, grad_fn=<MseLossBackward0>)\n",
      "349 tensor(3440484.2500, grad_fn=<MseLossBackward0>)\n",
      "350 tensor(3440478.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440478.2500, grad_fn=<MseLossBackward0>)\n",
      "351 tensor(3440472.5000, grad_fn=<MseLossBackward0>)\n",
      "352 tensor(3440467., grad_fn=<MseLossBackward0>)\n",
      "353 tensor(3440462., grad_fn=<MseLossBackward0>)\n",
      "354 tensor(3440455.7500, grad_fn=<MseLossBackward0>)\n",
      "355 tensor(3440450.2500, grad_fn=<MseLossBackward0>)\n",
      "356 tensor(3440444.5000, grad_fn=<MseLossBackward0>)\n",
      "357 tensor(3440439., grad_fn=<MseLossBackward0>)\n",
      "358 tensor(3440433.5000, grad_fn=<MseLossBackward0>)\n",
      "359 tensor(3440427.7500, grad_fn=<MseLossBackward0>)\n",
      "360 tensor(3440422.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440422.2500, grad_fn=<MseLossBackward0>)\n",
      "361 tensor(3440416.5000, grad_fn=<MseLossBackward0>)\n",
      "362 tensor(3440411., grad_fn=<MseLossBackward0>)\n",
      "363 tensor(3440405.5000, grad_fn=<MseLossBackward0>)\n",
      "364 tensor(3440399.7500, grad_fn=<MseLossBackward0>)\n",
      "365 tensor(3440394.7500, grad_fn=<MseLossBackward0>)\n",
      "366 tensor(3440388.7500, grad_fn=<MseLossBackward0>)\n",
      "367 tensor(3440383.5000, grad_fn=<MseLossBackward0>)\n",
      "368 tensor(3440377.5000, grad_fn=<MseLossBackward0>)\n",
      "369 tensor(3440372.2500, grad_fn=<MseLossBackward0>)\n",
      "370 tensor(3440366.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440366.2500, grad_fn=<MseLossBackward0>)\n",
      "371 tensor(3440360.7500, grad_fn=<MseLossBackward0>)\n",
      "372 tensor(3440355.7500, grad_fn=<MseLossBackward0>)\n",
      "373 tensor(3440350., grad_fn=<MseLossBackward0>)\n",
      "374 tensor(3440344.5000, grad_fn=<MseLossBackward0>)\n",
      "375 tensor(3440338.7500, grad_fn=<MseLossBackward0>)\n",
      "376 tensor(3440333.2500, grad_fn=<MseLossBackward0>)\n",
      "377 tensor(3440327.7500, grad_fn=<MseLossBackward0>)\n",
      "378 tensor(3440322., grad_fn=<MseLossBackward0>)\n",
      "379 tensor(3440316.5000, grad_fn=<MseLossBackward0>)\n",
      "380 tensor(3440310.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440310.7500, grad_fn=<MseLossBackward0>)\n",
      "381 tensor(3440305.2500, grad_fn=<MseLossBackward0>)\n",
      "382 tensor(3440300.2500, grad_fn=<MseLossBackward0>)\n",
      "383 tensor(3440294.5000, grad_fn=<MseLossBackward0>)\n",
      "384 tensor(3440289., grad_fn=<MseLossBackward0>)\n",
      "385 tensor(3440283.2500, grad_fn=<MseLossBackward0>)\n",
      "386 tensor(3440277.7500, grad_fn=<MseLossBackward0>)\n",
      "387 tensor(3440272.2500, grad_fn=<MseLossBackward0>)\n",
      "388 tensor(3440266.7500, grad_fn=<MseLossBackward0>)\n",
      "389 tensor(3440261.2500, grad_fn=<MseLossBackward0>)\n",
      "390 tensor(3440255.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440255.5000, grad_fn=<MseLossBackward0>)\n",
      "391 tensor(3440250., grad_fn=<MseLossBackward0>)\n",
      "392 tensor(3440244.5000, grad_fn=<MseLossBackward0>)\n",
      "393 tensor(3440239., grad_fn=<MseLossBackward0>)\n",
      "394 tensor(3440233.7500, grad_fn=<MseLossBackward0>)\n",
      "395 tensor(3440227.7500, grad_fn=<MseLossBackward0>)\n",
      "396 tensor(3440222.7500, grad_fn=<MseLossBackward0>)\n",
      "397 tensor(3440217., grad_fn=<MseLossBackward0>)\n",
      "398 tensor(3440211.5000, grad_fn=<MseLossBackward0>)\n",
      "399 tensor(3440206.2500, grad_fn=<MseLossBackward0>)\n",
      "400 tensor(3440200.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3440200.5000, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GCN()\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Define optimizer.\n",
    "\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(401):\n",
    "    \n",
    "    loss, h = train(data)\n",
    "    print(epoch, loss)\n",
    "    if epoch % 10 == 0:\n",
    "        #visualize_embedding(h, color=data.y, epoch=epoch, loss=loss)\n",
    "        print(loss)\n",
    "        time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ff565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
